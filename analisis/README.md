# AnÃ¡lisis de Rendimiento FIO: Baseline VM vs ML

AnÃ¡lisis comparativo completo de experimentos de I/O usando FIO entre una lÃ­nea base (VM) y una implementaciÃ³n basada en red neuronal.

## ğŸ“ Estructura

```
analisis/
â”œâ”€â”€ baseline/              # AnÃ¡lisis lÃ­nea base (VM)
â”‚   â”œâ”€â”€ throughput_lectura.png
â”‚   â”œâ”€â”€ latencia_p99.png
â”‚   â”œâ”€â”€ resultados_detalle.csv
â”‚   â”œâ”€â”€ resumen_metricas.csv
â”‚   â””â”€â”€ reporte_baseline.md
â”œâ”€â”€ ml/                    # AnÃ¡lisis Red Neuronal
â”‚   â”œâ”€â”€ throughput_lectura.png
â”‚   â”œâ”€â”€ latencia_p99.png
â”‚   â”œâ”€â”€ resultados_detalle.csv
â”‚   â”œâ”€â”€ resumen_metricas.csv
â”‚   â””â”€â”€ reporte_ml.md
â”œâ”€â”€ comparativa/           # AnÃ¡lisis Comparativo
â”‚   â”œâ”€â”€ comparativa_metricas.png
â”‚   â”œâ”€â”€ resultados_combinados.csv
â”‚   â”œâ”€â”€ comparativa_metricas.csv
â”‚   â””â”€â”€ reporte_comparativa.md
â”œâ”€â”€ analizar.py           # Script principal
â””â”€â”€ README.md             # Este archivo
```

## ğŸš€ EjecuciÃ³n RÃ¡pida

```powershell
python analisis/analizar.py
```

El script ejecutarÃ¡ todo el pipeline automÃ¡ticamente y generarÃ¡:

- 2 grÃ¡ficos por implementaciÃ³n (throughput + latencia)
- 1 grÃ¡fico comparativo side-by-side
- Reportes markdown con hallazgos
- Archivos CSV con datos detallados

## ğŸ“Š GrÃ¡ficos Generados

### Baseline (VM)

- **throughput_lectura.png**: Throughput de lectura por patrÃ³n y tamaÃ±o
- **latencia_p99.png**: Latencia p99 de lectura

### ML (Red Neuronal)

- **throughput_lectura.png**: Throughput de lectura (ML)
- **latencia_p99.png**: Latencia p99 de lectura (ML)

### Comparativa

- **comparativa_metricas.png**: Throughput y latencia side-by-side (Baseline vs ML)

## ğŸ“ˆ InterpretaciÃ³n de Resultados

### Throughput (MB/s)

Mayor es mejor. Compara velocidad de lectura en diferentes patrones:

- **SEQ**: Acceso secuencial (mejor caso)
- **RAND**: Acceso aleatorio (peor caso)
- **MIX**: Acceso mixto

### Latencia p99 (ms)

Menor es mejor. Percentil 99 del tiempo de respuesta.

## ğŸ“„ Reportes Markdown

- `baseline/reporte_baseline.md`: Resumen de hallazgos Baseline
- `ml/reporte_ml.md`: Resumen de hallazgos ML
- `comparativa/reporte_comparativa.md`: AnÃ¡lisis comparativo detallado

## ğŸ“Š Archivos CSV

### Detalle

- `resultados_detalle.csv`: MÃ©trica por corrida individual
- Columnas: workload, size_label, run, op, bw_MB_s, iops, p99_ms, lat_mean_ms

### Resumen

- `resumen_metricas.csv`: Agregados por patrÃ³n/tamaÃ±o/operaciÃ³n
- Columnas: workload, op, size_label, runs, bw_MB_s_mean, bw_MB_s_std, p99_ms_mean

### Comparativa

- `comparativa_metricas.csv`: Datos combinados con implementaciÃ³n
- `resultados_combinados.csv`: Detalle completo con etiqueta de implementaciÃ³n

## ğŸ”§ Requisitos

```powershell
pip install pandas numpy matplotlib seaborn
```

## ğŸ“ Hallazgos Clave

Los reportes markdown en cada carpeta contienen:

- Resumen ejecutivo con mÃ©tricas principales
- Desglose por patrÃ³n de acceso (SEQ, RAND, MIX)
- AnÃ¡lisis comparativo con deltas porcentuales

## ğŸ¯ PrÃ³ximos Pasos

1. Revisar grÃ¡ficos en cada carpeta
2. Leer reportes markdown para interpretaciÃ³n
3. Analizar CSV con herramientas adicionales si se requiere
4. Comparar resultados entre baseline y ML en carpeta `comparativa/`

## ğŸš€ Inicio RÃ¡pido

### Prerequisitos

AsegÃºrate de tener instaladas las siguientes librerÃ­as de Python:

```powershell
pip install pandas numpy matplotlib seaborn
```

O instala todas las dependencias desde el archivo de requisitos del proyecto:

```powershell
pip install -r ../requirements.txt
```

### EjecuciÃ³n del AnÃ¡lisis Completo

Para ejecutar todo el pipeline de anÃ¡lisis de una vez:

```powershell
cd analisis
python run_analysis.py
```

Este comando ejecutarÃ¡:

1. Procesamiento de todos los archivos JSON de resultados
2. GeneraciÃ³n de estadÃ­sticas agregadas
3. CreaciÃ³n de todas las grÃ¡ficas
4. GeneraciÃ³n del reporte de hallazgos

## ğŸ“Š EjecuciÃ³n MÃ³dulo por MÃ³dulo

Si prefieres ejecutar cada componente por separado:

### 1. Procesar Resultados

```powershell
python process_results.py
```

**Salidas generadas:**

- `processed_results.csv`: Dataset completo con todos los experimentos procesados
- `statistics_summary.csv`: EstadÃ­sticas agregadas por configuraciÃ³n

### 2. Generar GrÃ¡ficas

```powershell
python generate_plots.py
```

**GrÃ¡ficas generadas:**

- `iops_comparison.png`: ComparaciÃ³n de IOPS por tipo de acceso y tamaÃ±o
- `bandwidth_comparison.png`: AnÃ¡lisis de ancho de banda
- `latency_analysis.png`: AnÃ¡lisis detallado de latencia (4 subgrÃ¡ficas)
- `throughput_efficiency.png`: Eficiencia de throughput
- `performance_heatmap.png`: Mapas de calor de rendimiento (4 mÃ©tricas)
- `comparative_radar.png`: GrÃ¡fica radar comparativa de rendimiento normalizado
- `variability_analysis.png`: AnÃ¡lisis de consistencia entre runs (4 subgrÃ¡ficas)
- `percentile_latency.png`: Percentiles de latencia (P50, P95, P99)

### 3. Generar Reporte

```powershell
python generate_report.py
```

**Salida generada:**

- `REPORTE_HALLAZGOS.md`: Reporte completo en formato Markdown con:
  - Resumen ejecutivo
  - MÃ©tricas principales
  - AnÃ¡lisis por tipo de acceso
  - AnÃ¡lisis de escalabilidad
  - AnÃ¡lisis de latencia
  - AnÃ¡lisis de variabilidad
  - Hallazgos clave
  - Recomendaciones
  - Conclusiones

## ğŸ“ˆ Estructura de Datos

### Datos de Entrada

Los scripts procesan los resultados ubicados en:

```
../experiments/results_baseline/
â”œâ”€â”€ seq/
â”‚   â”œâ”€â”€ 100M/
â”‚   â”‚   â”œâ”€â”€ result_100M_run1.json
â”‚   â”‚   â”œâ”€â”€ result_100M_run2.json
â”‚   â”‚   â””â”€â”€ result_100M_run3.json
â”‚   â”œâ”€â”€ 500M/
â”‚   â””â”€â”€ 1G/
â”œâ”€â”€ rand/
â”‚   â”œâ”€â”€ 100M/
â”‚   â”œâ”€â”€ 500M/
â”‚   â””â”€â”€ 1G/
â””â”€â”€ mix/
    â”œâ”€â”€ 100M/
    â”œâ”€â”€ 500M/
    â””â”€â”€ 1G/
```

### ConfiguraciÃ³n de Experimentos

- **Patrones de acceso**: Secuencial (seq), Aleatorio (rand), Mixto (mix)
- **TamaÃ±os de archivo**: 100M, 500M, 1G
- **Repeticiones**: 3 runs por configuraciÃ³n
- **Total de experimentos**: 27 ejecuciones (3 Ã— 3 Ã— 3)

### MÃ©tricas Analizadas

#### Principales

- **IOPS** (I/O Operations Per Second): Operaciones de entrada/salida por segundo
- **Ancho de Banda** (MB/s): Throughput en megabytes por segundo
- **Latencia** (Î¼s): Tiempo de respuesta en microsegundos

#### MÃ©tricas Detalladas

- Latencia media, mÃ­nima, mÃ¡xima y desviaciÃ³n estÃ¡ndar
- Percentiles de latencia (P50, P95, P99)
- Total de datos procesados
- Throughput efectivo
- Coeficiente de variaciÃ³n

## ğŸ” InterpretaciÃ³n de Resultados

### IOPS (Input/Output Operations Per Second)

- **Mayor es mejor**
- Indica cuÃ¡ntas operaciones de I/O puede procesar el sistema por segundo
- CrÃ­tico para aplicaciones con muchas operaciones pequeÃ±as

### Ancho de Banda (Bandwidth)

- **Mayor es mejor**
- Medido en MB/s
- Indica la cantidad de datos que pueden transferirse por unidad de tiempo
- Importante para operaciones con archivos grandes

### Latencia

- **Menor es mejor**
- Medida en microsegundos (Î¼s)
- Tiempo que tarda en completarse una operaciÃ³n de I/O
- Los percentiles altos (P95, P99) son crÃ­ticos para detectar outliers

### Coeficiente de VariaciÃ³n (CV)

- Mide la consistencia de los resultados entre ejecuciones
- **CV < 5%**: Excelente consistencia
- **CV 5-10%**: Buena consistencia
- **CV 10-15%**: Consistencia moderada
- **CV > 15%**: Alta variabilidad

## ğŸ“¦ Archivos Generados

DespuÃ©s de ejecutar el anÃ¡lisis completo, encontrarÃ¡s:

```
analisis/
â”œâ”€â”€ process_results.py
â”œâ”€â”€ generate_plots.py
â”œâ”€â”€ generate_report.py
â”œâ”€â”€ run_analysis.py
â”œâ”€â”€ README.md
â”œâ”€â”€ processed_results.csv           # Dataset procesado
â”œâ”€â”€ statistics_summary.csv          # EstadÃ­sticas agregadas
â”œâ”€â”€ REPORTE_HALLAZGOS.md           # Reporte de hallazgos
â”œâ”€â”€ iops_comparison.png
â”œâ”€â”€ bandwidth_comparison.png
â”œâ”€â”€ latency_analysis.png
â”œâ”€â”€ throughput_efficiency.png
â”œâ”€â”€ performance_heatmap.png
â”œâ”€â”€ comparative_radar.png
â”œâ”€â”€ variability_analysis.png
â””â”€â”€ percentile_latency.png
```

## ğŸ› ï¸ PersonalizaciÃ³n

### Modificar Rutas

Si tus resultados estÃ¡n en una ubicaciÃ³n diferente, modifica la variable `results_path` en cada script:

```python
results_path = Path('ruta/a/tus/resultados')
```

### Agregar Nuevas MÃ©tricas

1. Edita `process_results.py` para extraer mÃ©tricas adicionales del JSON
2. Actualiza `generate_plots.py` para visualizar las nuevas mÃ©tricas
3. Modifica `generate_report.py` para incluirlas en el reporte

### Cambiar Estilo de GrÃ¡ficas

En `generate_plots.py`, ajusta los parÃ¡metros de estilo:

```python
plt.style.use('seaborn-v0_8-darkgrid')  # Cambiar estilo
sns.set_palette("husl")                  # Cambiar paleta de colores
plt.rcParams['figure.figsize'] = (12, 8) # Ajustar tamaÃ±o
```

## ğŸ“Š Visualizaciones Disponibles

### 1. IOPS Comparison

- Barras agrupadas y box plots
- Compara IOPS entre patrones de acceso y tamaÃ±os

### 2. Bandwidth Comparison

- Barras y grÃ¡ficas de lÃ­nea
- Muestra tendencias de ancho de banda

### 3. Latency Analysis

- 4 subgrÃ¡ficas con anÃ¡lisis completo
- Incluye distribuciÃ³n, variabilidad y rangos

### 4. Throughput Efficiency

- Throughput efectivo y datos procesados
- EvalÃºa eficiencia del sistema

### 5. Performance Heatmap

- 4 mapas de calor
- VisualizaciÃ³n rÃ¡pida de patrones de rendimiento

### 6. Comparative Radar

- GrÃ¡fica tipo radar
- Compara rendimiento normalizado entre patrones

### 7. Variability Analysis

- 4 subgrÃ¡ficas sobre consistencia
- EvalÃºa variabilidad entre runs

### 8. Percentile Latency

- Percentiles P50, P95 y P99
- Identifica latencias extremas

## ğŸ› SoluciÃ³n de Problemas

### Error: "No module named 'pandas'"

```powershell
pip install pandas numpy matplotlib seaborn
```

### Error: "File not found"

Verifica que los resultados estÃ©n en la ruta correcta:

```
../experiments/results_baseline/
```

### GrÃ¡ficas no se generan

AsegÃºrate de haber ejecutado primero `process_results.py` o `run_analysis.py`

### Warnings sobre seaborn

Si ves warnings sobre estilos de seaborn, es normal. Las grÃ¡ficas se generarÃ¡n correctamente.

## ğŸ“ Notas Adicionales

- Todos los scripts estÃ¡n diseÃ±ados para ejecutarse de forma independiente o conjunta
- Los resultados son reproducibles gracias al procesamiento determinÃ­stico
- Las grÃ¡ficas se generan en alta resoluciÃ³n (300 DPI) para publicaciones
- El reporte estÃ¡ en formato Markdown, fÃ¡cil de convertir a PDF o HTML

## ğŸ“ Contacto y Soporte

Para preguntas o problemas relacionados con el anÃ¡lisis:

- Revisa el cÃ³digo fuente de los scripts (estÃ¡n bien comentados)
- Consulta el reporte de hallazgos generado
- Verifica que todos los prerequisitos estÃ©n instalados

## ğŸ”„ ActualizaciÃ³n de AnÃ¡lisis

Para actualizar el anÃ¡lisis con nuevos resultados:

1. AsegÃºrate de que los nuevos archivos JSON estÃ©n en la estructura correcta
2. Ejecuta nuevamente:
   ```powershell
   python run_analysis.py
   ```
3. Los archivos existentes serÃ¡n sobrescritos con los nuevos resultados

## ğŸ“„ Licencia

Este cÃ³digo de anÃ¡lisis es parte del proyecto SO-Kernel-trace-extraction.

---

**Ãšltima actualizaciÃ³n:** Diciembre 2025
