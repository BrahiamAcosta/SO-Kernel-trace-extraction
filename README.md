# SO-Kernel-trace-extraction
## Pipeline de ML para ClasificaciÃ³n de Patrones de I/O

Sistema de aprendizaje automÃ¡tico para clasificar patrones de acceso a disco (secuencial, aleatorio, mixto) y optimizar el readahead en el kernel Linux mediante KML (Kernel Machine Learning).

---

## ğŸ“‹ Resumen Ejecutivo

Este proyecto desarrolla un componente de red neuronal que clasifica patrones de I/O en tiempo real dentro del kernel Linux. El objetivo es predecir el tipo de patrÃ³n de acceso (sequential, random, mixed) para ajustar dinÃ¡micamente el valor de readahead y mejorar el rendimiento del sistema de archivos.

**Contexto del Proyecto:** IntegraciÃ³n de ML en el kernel Linux usando el framework KML (Kernel Machine Learning) para ajustar automÃ¡ticamente parÃ¡metros de I/O en tiempo real.

**VersiÃ³n del Dataset:** Ventanas de 2.5 segundos (optimizado para balance entre granularidad y estabilidad)

### Flujo General del Proyecto

```
1. Dataset consolidado (CSV con caracterÃ­sticas pre-calculadas)
   â†“
2. Procesamiento y normalizaciÃ³n de datos
   â†“
3. Entrenamiento de red neuronal ligera
   â†“
4. ExportaciÃ³n a formato TorchScript
   â†“
5. IntegraciÃ³n en kernel Linux mediante KML
   â†“
6. Inferencia en tiempo real para ajustar readahead
```

**Tu responsabilidad**: Pasos 1-4 (desarrollo del modelo ML)  
**CompaÃ±ero**: Pasos 5-6 (integraciÃ³n en kernel)

---

## ğŸ“Š DescripciÃ³n del Dataset

### Metadata del Dataset

| Propiedad | Valor |
|-----------|-------|
| **Archivo** | `consolidated_dataset.csv` |
| **Filas totales** | ~866 (18 runs Ã— 48 ventanas) |
| **Columnas** | 40 features + 1 label |
| **Tipo de problema** | ClasificaciÃ³n multiclase (3 clases) |
| **Clases balanceadas** | SÃ­: 288 filas por clase |
| **Granularidad temporal** | Ventanas de 2.5 segundos |
| **DuraciÃ³n por run** | 120 segundos (48 ventanas por run) |
| **Formato** | CSV con header |

### Origen de los Datos

Los datos fueron capturados mediante:
- **LTTng (Linux Trace Toolkit)**: Captura de eventos del kernel (block layer, I/O scheduler, page cache)
- **FIO (Flexible I/O Tester)**: GeneraciÃ³n de cargas sintÃ©ticas de trabajo controladas
- **Sistema**: Ubuntu Server con kernel Linux 6.x
- **Hardware**: Disco de prueba, 2 CPU cores, memoria variable

**ConfiguraciÃ³n de captura:**
- Runtime por patrÃ³n: 120 segundos
- Repeticiones: 3 runs por patrÃ³n
- Modos: cold (cache vacÃ­o) y warm (cache pre-cargado)
- Direct I/O: habilitado (bypass page cache)

### DistribuciÃ³n de Clases

```
Label         Filas    DescripciÃ³n                          ParÃ¡metros FIO
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sequential    288      Acceso secuencial                    bs=128k, iodepth=4
                       (lectura lineal continua)            
                       
random        288      Acceso aleatorio                     bs=4k, iodepth=16
                       (random reads 4KB)                   
                       
mixed         288      PatrÃ³n mixto                         bs=64k, iodepth=8
                       (70% read, 30% write, randrw)        
```

**Balance perfecto:** 288 samples por clase (33.3% cada una) - no requiere tÃ©cnicas de balanceo.

### JustificaciÃ³n de Ventanas de 2.5 Segundos

| Criterio | 5 segundos | **2.5 segundos** â­ | 1 segundo |
|----------|------------|---------------------|-----------|
| **Samples totales** | 433 | **866** | 2,160 |
| **Estabilidad estadÃ­stica** | Muy alta | **Alta** | Media-Baja |
| **Capacidad de reacciÃ³n** | Lenta (5s lag) | **Balanceada (2.5s lag)** | RÃ¡pida (1s lag) |
| **Overhead en producciÃ³n** | Muy bajo | **Bajo** | Medio-Alto |
| **Riesgo de thrashing** | Muy bajo | **Bajo** | Alto |
| **Adecuado para entrenar** | Ajustado | **Ã“ptimo** | Excelente |
| **Realismo en producciÃ³n** | Conservador | **PrÃ¡ctico** | Agresivo |

**ConclusiÃ³n:** 2.5 segundos ofrece el mejor balance entre suficientes datos para entrenar modelos robustos, features estadÃ­sticamente significativas, latencia de adaptaciÃ³n aceptable y bajo overhead computacional.

### CaracterÃ­sticas CrÃ­ticas del Dataset

Las **5 caracterÃ­sticas** seleccionadas para el modelo son:

1. **`trace_avg_sector_distance`** (Feature #1 en importancia) â­â­â­â­â­
   - Distancia promedio entre sectores consecutivos (sectores de 512B)
   - **Sequential:** ~5-20 sectores (4-8 KB)
   - **Random:** ~30,000-80,000 sectores (20+ MB)
   - **Mixed:** ~500-5,000 sectores (500KB-1.5MB)

2. **`trace_sector_jump_ratio`** (Feature #2 en importancia) â­â­â­â­â­
   - Ratio de saltos >1MB entre accesos consecutivos (0.0-1.0)
   - **Sequential:** ~0.00-0.10 (1-5%)
   - **Random:** ~0.80-0.98 (85-95%)
   - **Mixed:** ~0.30-0.60 (40-60%)

3. **`bw_mean_kbps`** (Bandwidth promedio) â­â­â­â­
   - **Sequential:** ~400,000-600,000 KB/s (400-600 MB/s)
   - **Random:** ~8,000-20,000 KB/s (10-20 MB/s)
   - **Mixed:** ~100,000-300,000 KB/s (100-300 MB/s)

4. **`lat_mean_ns`** (Latencia promedio) â­â­â­â­
   - **Sequential:** ~2,000,000-3,000,000 ns (2-3 ms)
   - **Random:** ~8,000,000-15,000,000 ns (8-15 ms)
   - **Mixed:** ~4,000,000-8,000,000 ns (4-8 ms)

5. **`iops_mean`** (IOPS promedio) â­â­â­
   - **Sequential:** ~300-500 ops/s
   - **Random:** ~2,000-5,000 ops/s
   - **Mixed:** ~1,000-3,000 ops/s

**âš ï¸ IMPORTANTE:** Las caracterÃ­sticas `bs` y `iodepth` estÃ¡n **excluidas** del modelo porque estÃ¡n fuertemente correlacionadas con el label (riesgo de data leakage). En producciÃ³n, no conocerÃ¡s estos parÃ¡metros, solo puedes observar el comportamiento.

---

## ğŸ—ï¸ Estructura del CÃ³digo

### Archivos Principales

#### `build_dataset_from_consolidated.py`
**Â¿QuÃ© hace?**  
Procesa el dataset consolidado (`consolidated_dataset.csv`) y prepara los datos para entrenamiento.

**Funcionamiento:**
1. Lee el CSV con caracterÃ­sticas ya calculadas por ventana
2. Mapea las columnas del CSV a las 5 caracterÃ­sticas que necesita el modelo:
   - `trace_avg_sector_distance * 512` â†’ Distancia promedio (bytes)
   - `trace_sector_jump_ratio` â†’ Variabilidad
   - `bw_mean_kbps / iops_mean` â†’ TamaÃ±o promedio I/O (bytes)
   - `1 - trace_sector_jump_ratio` â†’ Ratio secuencial
   - `iops_mean` â†’ Tasa de I/O (IOPS)
3. Mapea etiquetas de texto (`sequential`, `random`, `mixed`) a nÃºmeros (0, 1, 2)
4. Divide los datos en train/test (80/20) de forma estratificada
5. Normaliza las caracterÃ­sticas usando `StandardScaler`
6. Guarda:
   - `data/processed/train.npz` y `test.npz` (datos normalizados)
   - `artifacts/scaler.pkl` (normalizador - **CRÃTICO para kernel**)
   - `artifacts/metadata.json` (metadatos del dataset)

**Por quÃ© estas 5 caracterÃ­sticas?**  
Capturan los aspectos distintivos de cada patrÃ³n de forma eficiente y son computacionalmente baratas de calcular en tiempo real dentro del kernel.

#### `neuronal_red.py`
**Â¿QuÃ© hace?**  
Define la arquitectura de la red neuronal.

**Arquitectura:**
```python
Input (5 caracterÃ­sticas) 
  â†’ Capa Densa 1: 5 â†’ 32 neuronas + ReLU + Dropout(20%)
  â†’ Capa Densa 2: 32 â†’ 16 neuronas + ReLU
  â†’ Capa Densa 3: 16 â†’ 3 neuronas (logits)
  â†’ Salida: [score_sequential, score_random, score_mixed]
```

**Â¿Por quÃ© es "ligera"?**
- Solo 3 capas densas (no es una red profunda)
- MÃ¡ximo 32 neuronas por capa
- TamaÃ±o total: ~15 KB
- Inferencia rÃ¡pida (microsegundos)
- Optimizada para ejecuciÃ³n en kernel donde los recursos son limitados

**Componentes:**
- `ReLU`: FunciÃ³n de activaciÃ³n que introduce no-linealidad
- `Dropout(0.2)`: RegularizaciÃ³n que previene sobreajuste (desactiva 20% de neuronas aleatoriamente durante entrenamiento)
- `CrossEntropyLoss`: FunciÃ³n de pÃ©rdida para clasificaciÃ³n multi-clase

#### `train.py`
**Â¿QuÃ© hace?**  
Entrena la red neuronal y exporta el modelo en formatos compatibles con el kernel.

**Proceso de entrenamiento:**
1. Carga los datos de entrenamiento y prueba
2. Crea un `DataLoader` con batches de 128 muestras
3. Inicializa el modelo, optimizador (Adam) y funciÃ³n de pÃ©rdida
4. Entrena durante hasta 60 Ã©pocas con:
   - **Early stopping**: Se detiene si no mejora en 8 Ã©pocas consecutivas
   - **ValidaciÃ³n**: EvalÃºa en el conjunto de prueba cada Ã©poca
   - **Mejor modelo**: Guarda el modelo con mejor accuracy en validaciÃ³n
5. Exporta el modelo en dos formatos:
   - `model.pth`: Pesos PyTorch (para Python)
   - `model_ts.pt`: **TorchScript** (para C/C++ y kernel) â­ **PRINCIPAL**
   - `model.onnx`: ONNX (opcional, si se requiere)

**ParÃ¡metros de entrenamiento:**
- Learning rate: 0.001
- Batch size: 128
- Optimizador: Adam
- Early stopping: Paciencia de 8 Ã©pocas

#### `evaluate.py`
**Â¿QuÃ© hace?**  
EvalÃºa el modelo entrenado y genera mÃ©tricas de rendimiento.

**MÃ©tricas generadas:**
- Accuracy general
- Matriz de confusiÃ³n (muestra errores por clase)
- Guarda resultados en `artifacts/eval_summary.json`

---

## ğŸš€ CÃ³mo Ejecutar el Pipeline Completo

### 1. InstalaciÃ³n de Dependencias

```bash
pip install -r requirements.txt
```

**Dependencias principales:**
- `torch`: Framework de deep learning
- `numpy`, `pandas`: ManipulaciÃ³n de datos
- `scikit-learn`: NormalizaciÃ³n y divisiÃ³n de datos
- `joblib`: Guardar/cargar el normalizador

### 2. Preparar el Dataset

**Requisitos del CSV:**
- Archivo: `consolidated_dataset.csv` en el directorio raÃ­z
- Debe tener una columna `label` con valores: `sequential`, `random`, `mixed`
- Debe contener las columnas necesarias para calcular las 5 caracterÃ­sticas

**Ejecutar:**
```bash
python build_dataset_from_consolidated.py
```

**Salida esperada:**
```
Dataset procesado exitosamente!
  - Train: 691 muestras
  - Test: 173 muestras
  - Features: 5
  - Clases: 3
```

**Archivos generados:**
- `data/processed/train.npz` - Datos de entrenamiento normalizados
- `data/processed/test.npz` - Datos de prueba normalizados
- `artifacts/scaler.pkl` - Normalizador (necesario para kernel)
- `artifacts/metadata.json` - Metadatos del dataset

### 3. Entrenar el Modelo

```bash
python train.py
```

**Salida esperada:**
```
Epoch 001 | loss=1.0957 | val_acc=0.3353
Epoch 002 | loss=1.0646 | val_acc=0.3353
...
Epoch 031 | loss=0.1378 | val_acc=0.9711
Early stopping por paciencia.
Entrenamiento completo. Accuracy test=0.9711. Artefactos en 'artifacts/'.
```

**Archivos generados:**
- `artifacts/model.pth` - Pesos PyTorch
- `artifacts/model_ts.pt` - **TorchScript (PARA KERNEL)** â­
- `artifacts/training_summary.json` - Resumen del entrenamiento

### 4. Evaluar el Modelo

```bash
python evaluate.py
```

**Salida esperada:**
```
Accuracy test: 0.9711
Matriz de confusiÃ³n (filas=verdadero, columnas=predicho):
[[55  0  2]
 [ 0 58  0]
 [ 1  2 55]]
```

**Archivo generado:**
- `artifacts/eval_summary.json` - MÃ©tricas de evaluaciÃ³n

---

## ğŸ“Š Resultados del Modelo

- **Accuracy en test**: 97.11%
- **DistribuciÃ³n de clases**: Balanceada (288 muestras por clase)
- **TamaÃ±o del modelo**: ~15 KB (TorchScript)
- **Tiempo de inferencia**: Microsegundos (optimizado para kernel)

### Matriz de ConfusiÃ³n
```
                Predicho
              Seq  Rand  Mix
Real Seq       55    0    2
Real Rand       0   58    0
Real Mixed      1    2   55
```

- **Sequential**: 96.5% correctos
- **Random**: 100% correctos
- **Mixed**: 94.8% correctos

---

## ğŸ”® CÃ³mo Usar el Modelo para Hacer Predicciones

**âš ï¸ IMPORTANTE:** La red neuronal **NO** calcula las caracterÃ­sticas automÃ¡ticamente. TÃº debes:
1. **Calcular las 5 caracterÃ­sticas** desde tus datos de I/O raw
2. **Normalizar** las caracterÃ­sticas usando el scaler
3. **Pasar** las caracterÃ­sticas normalizadas a la red neuronal

La red neuronal solo recibe las 5 caracterÃ­sticas ya calculadas y las clasifica.

### Ejemplo RÃ¡pido con Python

```python
import joblib
import numpy as np
import torch
from neuronal_red import IOPatternClassifier

# 1. Cargar modelo y scaler
scaler = joblib.load("artifacts/scaler.pkl")
model = IOPatternClassifier(input_size=5, hidden_size=32, num_classes=3)
model.load_state_dict(torch.load("artifacts/model.pth", map_location="cpu"))
model.eval()

# 2. Preparar caracterÃ­sticas (ejemplo: patrÃ³n secuencial)
# âš ï¸ NOTA: Estas caracterÃ­sticas DEBEN calcularse desde tus datos de I/O raw
# La red neuronal NO las calcula automÃ¡ticamente
# 
# Ejemplo de cÃ¡lculo:
# - offsets = [0, 131072, 262144, ...]  # offsets de operaciones I/O
# - avg_distance = promedio(|offsets[i+1] - offsets[i]|) * 512
# - jump_ratio = % de saltos > 1MB
# - etc.
#
# AquÃ­ usamos valores ya calculados (basados en el dataset real):
features = np.array([
    102774272.0,   # [0] Distancia promedio: 200,731 sectores Ã— 512 bytes
    0.14,          # [1] Variabilidad (jump ratio)
    69436416.0,    # [2] TamaÃ±o promedio I/O: (67,809 KB/s Ã— 1024) / 1.0 IOPS
    0.86,          # [3] Ratio secuencial: 1.0 - 0.14
    1.0            # [4] IOPS
], dtype=np.float32)

# 3. CRÃTICO: Normalizar las caracterÃ­sticas
features_normalized = scaler.transform(features.reshape(1, -1))

# 4. Hacer predicciÃ³n
features_tensor = torch.tensor(features_normalized, dtype=torch.float32)
with torch.no_grad():
    logits = model(features_tensor)
    probabilities = torch.softmax(logits, dim=1)
    predicted_class = torch.argmax(logits, dim=1).item()

# 5. Interpretar resultado
class_map = {0: "sequential", 1: "random", 2: "mixed"}
predicted_label = class_map[predicted_class]
confidence = probabilities[0][predicted_class].item()

print(f"PredicciÃ³n: {predicted_label} (confianza: {confidence*100:.2f}%)")
```

### Script de Ejemplo Completo

Ejecuta el script `predict.py` para ver ejemplos completos con los tres patrones:

```bash
python predict.py
```

Este script muestra cÃ³mo hacer predicciones con valores reales del dataset.

### Valores de Ejemplo por PatrÃ³n

**Para probar con datos similares a los del entrenamiento, usa estos valores:**

#### PatrÃ³n Secuencial
```python
avg_sector_distance = 200731      # sectores
sector_jump_ratio = 0.14          # 14%
bw_mean_kbps = 67809             # KB/s
iops_mean = 1.0                   # ops/s
```

#### PatrÃ³n Aleatorio
```python
avg_sector_distance = 19534728    # sectores
sector_jump_ratio = 0.998          # 99.8%
bw_mean_kbps = 7518              # KB/s
iops_mean = 1.0                   # ops/s
```

#### PatrÃ³n Mixto
```python
avg_sector_distance = 7183669     # sectores
sector_jump_ratio = 0.994          # 99.4%
bw_mean_kbps = 39695             # KB/s
iops_mean = 1.0                   # ops/s
```

**âš ï¸ IMPORTANTE:** 
- Estos valores estÃ¡n basados en el dataset de entrenamiento
- En producciÃ³n, calcula las caracterÃ­sticas desde tus datos reales de I/O
- El IOPS en el dataset es constante (1.0), pero en producciÃ³n deberÃ­as calcular el IOPS real
- Siempre normaliza las caracterÃ­sticas antes de pasarlas al modelo

---

## ğŸ”§ IntegraciÃ³n en el Kernel Linux

### Contexto: Â¿QuÃ© necesita hacer tu compaÃ±ero?

El objetivo final es que el modelo se ejecute dentro del kernel Linux para clasificar patrones de I/O en tiempo real y ajustar el readahead dinÃ¡micamente.

### Archivos para Entregar

1. **`artifacts/model_ts.pt`** (14.8 KB) â­ **PRINCIPAL**
   - Modelo en formato TorchScript
   - Formato compatible con C/C++ y KML
   - Se carga directamente en el kernel

2. **`artifacts/scaler.pkl`** (719 bytes) â­ **CRÃTICO**
   - Contiene los parÃ¡metros de normalizaciÃ³n (medias y desviaciones estÃ¡ndar)
   - **NO se carga directamente**, pero sus parÃ¡metros deben implementarse en C
   - Las caracterÃ­sticas DEBEN normalizarse antes de cada inferencia

3. **`artifacts/metadata.json`**
   - Mapeo de clases: `{0: "sequential", 1: "random", 2: "mixed"}`
   - Dimensiones: 5 caracterÃ­sticas de entrada, 3 clases de salida
   - Referencia para implementaciÃ³n

### Proceso de IntegraciÃ³n (Responsabilidad del compaÃ±ero)

#### Paso 1: Cargar el Modelo TorchScript
- Usar la biblioteca de KML o wrapper de TorchScript para C
- Cargar `model_ts.pt` en memoria del kernel
- Inicializar el modelo para inferencia

#### Paso 2: Implementar NormalizaciÃ³n en C
- Extraer parÃ¡metros del `scaler.pkl` (medias y desviaciones estÃ¡ndar)
- Implementar normalizaciÃ³n en C:
  ```c
  normalized_feature[i] = (feature[i] - mean[i]) / std[i]
  ```
- Aplicar a las 5 caracterÃ­sticas antes de cada inferencia

#### Paso 3: Extraer CaracterÃ­sticas en Tiempo Real
- Interceptar operaciones de I/O en el kernel
- Calcular las 5 caracterÃ­sticas por ventana deslizante:
  1. Distancia promedio entre offsets
  2. Variabilidad (jump ratio)
  3. TamaÃ±o promedio de I/O
  4. Ratio secuencial
  5. IOPS
- Normalizar usando los parÃ¡metros del scaler

#### Paso 4: Ejecutar Inferencia
- Pasar las 5 caracterÃ­sticas normalizadas al modelo
- Obtener los 3 logits (scores) de salida
- Seleccionar la clase con mayor score

#### Paso 5: Mapear a Readahead
- Mapear clase predicha a valor de readahead:
  - `0 (sequential)` â†’ Readahead alto (ej: 128-256 KB)
  - `1 (random)` â†’ Readahead bajo (ej: 16-32 KB)
  - `2 (mixed)` â†’ Readahead intermedio (ej: 64-128 KB)
- Ajustar el readahead del sistema de archivos

### Consideraciones TÃ©cnicas para el Kernel

1. **Memoria limitada**: El modelo es ligero (~15 KB) para no consumir mucha memoria del kernel
2. **Latencia baja**: La inferencia debe ser rÃ¡pida (microsegundos) para no afectar el rendimiento
3. **NormalizaciÃ³n obligatoria**: Las caracterÃ­sticas DEBEN normalizarse igual que en entrenamiento
4. **Ventana deslizante**: Las caracterÃ­sticas se calculan sobre ventanas de operaciones de I/O
5. **Determinismo**: El modelo es determinÃ­stico (sin operaciones aleatorias) para comportamiento predecible

### Formato de Entrada para el Modelo

**Input**: Array de 5 valores float32 normalizados
```c
float features[5] = {
    normalized_avg_distance,
    normalized_variability,
    normalized_avg_io_size,
    normalized_seq_ratio,
    normalized_iops
};
```

**Output**: Array de 3 logits (scores)
```c
float logits[3] = {
    score_sequential,  // Clase 0
    score_random,      // Clase 1
    score_mixed        // Clase 2
};
// Clase predicha = Ã­ndice del mÃ¡ximo valor
```

### Valores Reales de Referencia para Predicciones

**âš ï¸ IMPORTANTE:** Estos son los valores reales del dataset de entrenamiento. Usa valores similares para obtener predicciones confiables.

#### PatrÃ³n Secuencial (Sequential)
```python
# Valores tÃ­picos basados en el dataset real:
avg_sector_distance = 200731      # Mediana: ~200,731 sectores (~100 MB)
sector_jump_ratio = 0.14           # Mediana: 0.14 (14% de saltos grandes)
bw_mean_kbps = 67809              # Mediana: ~67,809 KB/s (~66 MB/s)
iops_mean = 1.0                   # IOPS promedio

# CaracterÃ­sticas calculadas:
feature_1 = 200731 * 512 = 102,774,272 bytes      # Distancia promedio
feature_2 = 0.14                                   # Variabilidad
feature_3 = (67809 * 1024) / 1.0 = 69,436,416 bytes  # TamaÃ±o promedio I/O
feature_4 = 1.0 - 0.14 = 0.86                      # Ratio secuencial
feature_5 = 1.0                                    # IOPS
```

#### PatrÃ³n Aleatorio (Random)
```python
# Valores tÃ­picos basados en el dataset real:
avg_sector_distance = 19534728    # Mediana: ~19,534,728 sectores (~9.5 GB)
sector_jump_ratio = 0.998          # Mediana: 0.998 (99.8% de saltos grandes)
bw_mean_kbps = 7518              # Mediana: ~7,518 KB/s (~7.3 MB/s)
iops_mean = 1.0                  # IOPS promedio

# CaracterÃ­sticas calculadas:
feature_1 = 19534728 * 512 = 10,001,780,736 bytes    # Distancia promedio
feature_2 = 0.998                                     # Variabilidad
feature_3 = (7518 * 1024) / 1.0 = 7,698,432 bytes    # TamaÃ±o promedio I/O
feature_4 = 1.0 - 0.998 = 0.002                       # Ratio secuencial
feature_5 = 1.0                                       # IOPS
```

#### PatrÃ³n Mixto (Mixed)
```python
# Valores tÃ­picos basados en el dataset real:
avg_sector_distance = 7183669     # Mediana: ~7,183,669 sectores (~3.5 GB)
sector_jump_ratio = 0.994          # Mediana: 0.994 (99.4% de saltos grandes)
bw_mean_kbps = 39695             # Mediana: ~39,695 KB/s (~38.7 MB/s)
iops_mean = 1.0                  # IOPS promedio

# CaracterÃ­sticas calculadas:
feature_1 = 7183669 * 512 = 3,678,038,528 bytes      # Distancia promedio
feature_2 = 0.994                                    # Variabilidad
feature_3 = (39695 * 1024) / 1.0 = 40,647,680 bytes # TamaÃ±o promedio I/O
feature_4 = 1.0 - 0.994 = 0.006                      # Ratio secuencial
feature_5 = 1.0                                      # IOPS
```

**Rangos de Valores Observados en el Dataset:**

| CaracterÃ­stica | Sequential | Random | Mixed |
|----------------|------------|--------|-------|
| **Distancia promedio (sectores)** | 3,702 - 11,747,540 | 4,915 - 35,373,190 | 7,716 - 27,706,060 |
| **Jump ratio** | 0.00 - 0.45 | 0.76 - 1.00 | 0.96 - 1.00 |
| **Bandwidth (KB/s)** | 19,777 - 182,975 | 1,977 - 23,407 | 3,657 - 384,524 |
| **IOPS** | 1.0 (constante) | 1.0 (constante) | 1.0 (constante) |

**Nota sobre IOPS:** En este dataset, `iops_mean` es constante (1.0) para todas las muestras. En producciÃ³n, deberÃ­as calcular el IOPS real basado en el nÃºmero de operaciones por segundo observadas en tu ventana de tiempo.

---

## ğŸ”Œ Componentes de IntegraciÃ³n en el Kernel

### Archivos en `artifacts/`

El directorio `artifacts/` contiene todos los componentes necesarios para la integraciÃ³n:

#### 1. **Daemon ML Predictor** (`ml_predictor.cpp`)
- **DescripciÃ³n**: Daemon en C++ que carga el modelo TorchScript y ejecuta inferencias
- **Funcionalidad**: 
  - Escucha en un socket Unix (`/tmp/ml_predictor.sock`)
  - Recibe 5 caracterÃ­sticas normalizadas (5 floats)
  - Devuelve la clase predicha (1 int: 0, 1, o 2)
- **CompilaciÃ³n**: `make ml_predictor` (requiere libtorch)
- **Uso**: `./ml_predictor model_ts.pt`

#### 2. **eBPF Block Trace Collector** (`ebpf_block_trace.cpp`) â­ **NUEVO**
- **DescripciÃ³n**: Recolector de estadÃ­sticas I/O usando eBPF (tracepoints del kernel)
- **Funcionalidad**:
  - Captura eventos `block:block_rq_issue` usando eBPF
  - Agrega estadÃ­sticas en ventanas de tiempo (default: 2.5 segundos)
  - Calcula las 5 caracterÃ­sticas del modelo
  - EnvÃ­a caracterÃ­sticas al daemon ML predictor
  - Escribe readahead al sysfs segÃºn la predicciÃ³n
- **CompilaciÃ³n**: `make ebpf_block_trace` (requiere BCC)
- **Uso**: `sudo ./ebpf_block_trace --device nvme0n1 --window 2500`
- **MigraciÃ³n**: Migrado desde Python a C++ para unificar el stack tecnolÃ³gico
- **Ventajas**: Mejor rendimiento, menor overhead, mismo lenguaje que el daemon

#### 3. **MÃ³dulo del Kernel** (`ml_predictor.c`)
- **DescripciÃ³n**: MÃ³dulo del kernel para comunicaciÃ³n Netlink
- **Funcionalidad**: Permite que el kernel envÃ­e caracterÃ­sticas al daemon userspace
- **Estado**: Preparado para integraciÃ³n con el sistema de readahead del kernel

#### 4. **Scripts Alternativos**
- **`ml_feature_collector.sh`**: Script bash que usa `iostat` y `perf trace` (no requiere eBPF)
- **`ebpf_block_trace.py`**: VersiÃ³n Python original (puede mantenerse como fallback)

### Flujo de IntegraciÃ³n Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kernel Linux                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Block Layer (I/O requests)                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                                           â”‚
â”‚                 â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  eBPF Tracepoints (block_rq_issue)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Userspace                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ebpf_block_trace.cpp (C++)                          â”‚  â”‚
â”‚  â”‚  - Captura eventos eBPF                               â”‚  â”‚
â”‚  â”‚  - Agrega en ventanas de 2.5s                        â”‚  â”‚
â”‚  â”‚  - Calcula 5 caracterÃ­sticas                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                                           â”‚
â”‚                 â”‚ Unix Socket                               â”‚
â”‚                 â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ml_predictor.cpp (C++ Daemon)                       â”‚  â”‚
â”‚  â”‚  - Carga model_ts.pt                                  â”‚  â”‚
â”‚  â”‚  - Normaliza caracterÃ­sticas                          â”‚  â”‚
â”‚  â”‚  - Ejecuta inferencia                                 â”‚  â”‚
â”‚  â”‚  - Devuelve predicciÃ³n (0, 1, o 2)                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                                           â”‚
â”‚                 â”‚ Respuesta (int)                           â”‚
â”‚                 â–¼                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ebpf_block_trace.cpp                                 â”‚  â”‚
â”‚  â”‚  - Mapea predicciÃ³n a readahead_kb                    â”‚  â”‚
â”‚  â”‚  - Escribe a /sys/block/DEV/queue/read_ahead_kb       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Protocolo de ComunicaciÃ³n

**Socket Unix**: `/tmp/ml_predictor.sock`

1. **Cliente â†’ Daemon**: 5 floats (20 bytes, little-endian)
   ```c
   float features[5] = {
       avg_dist_bytes,      // [0] Distancia promedio (bytes)
       jump_ratio,           // [1] Variabilidad (0.0-1.0)
       avg_io_size_bytes,    // [2] TamaÃ±o promedio I/O (bytes)
       seq_ratio,            // [3] Ratio secuencial (0.0-1.0)
       iops_mean             // [4] IOPS (operaciones/segundo)
   };
   ```

2. **Daemon â†’ Cliente**: 1 int (4 bytes)
   ```c
   int prediction;  // 0=sequential, 1=random, 2=mixed
   ```

### Mapeo de Predicciones a Readahead

```c
int readahead_kb;
switch (prediction) {
    case 0:  // sequential
        readahead_kb = 256;
        break;
    case 1:  // random
        readahead_kb = 16;
        break;
    case 2:  // mixed
        readahead_kb = 64;
        break;
}
```

### CompilaciÃ³n de Componentes

```bash
cd artifacts

# Compilar todo
make

# O compilar individualmente
make ml_predictor        # Requiere libtorch
make ebpf_block_trace    # Requiere BCC
```

**Dependencias:**
- **libtorch**: Para `ml_predictor.cpp` (PyTorch C++ API)
- **BCC**: Para `ebpf_block_trace.cpp` (BPF Compiler Collection)
  - InstalaciÃ³n: `sudo apt-get install bpfcc-tools libbpfcc-dev`

### DocumentaciÃ³n Adicional

- **`MIGRACION_EBPF_CPP.md`**: Detalles sobre la migraciÃ³n de Python a C++
- **`CAMBIOS_APLICADOS.md`**: Historial de cambios en los scripts de integraciÃ³n

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "No se encontraron trazas en 'data/raw'"
- **SoluciÃ³n**: AsegÃºrate de tener `consolidated_dataset.csv` en el directorio raÃ­z

### Error: "No module named 'torch'"
- **SoluciÃ³n**: Instala las dependencias: `pip install -r requirements.txt`

### Warning: "exportaciÃ³n ONNX fallÃ³"
- **No es crÃ­tico**: TorchScript es el formato principal. ONNX es opcional.

### El modelo tiene baja accuracy
- Verifica que el dataset estÃ© balanceado
- Revisa que las caracterÃ­sticas se estÃ©n calculando correctamente
- Considera ajustar hiperparÃ¡metros en `train.py`

---

## ğŸ“ Notas Importantes

1. **NormalizaciÃ³n es CRÃTICA**: El modelo fue entrenado con datos normalizados. Sin normalizaciÃ³n, las predicciones serÃ¡n incorrectas.

2. **Orden de caracterÃ­sticas**: Las 5 caracterÃ­sticas deben pasarse en el mismo orden:
   - [0] Distancia promedio
   - [1] Variabilidad
   - [2] TamaÃ±o promedio I/O
   - [3] Ratio secuencial
   - [4] IOPS

3. **Ventana deslizante**: Las caracterÃ­sticas se calculan sobre ventanas de operaciones de I/O. El tamaÃ±o de ventana y overlap deben ser consistentes.

4. **TorchScript es el formato principal**: Aunque se exporta ONNX, TorchScript (`model_ts.pt`) es el formato recomendado para integraciÃ³n en kernel.

5. **ExclusiÃ³n de features con data leakage**: Las caracterÃ­sticas `bs` y `iodepth` estÃ¡n excluidas del modelo porque estÃ¡n fuertemente correlacionadas con el label. En producciÃ³n, solo puedes observar el comportamiento, no los parÃ¡metros de configuraciÃ³n.

---

## ğŸ“š Referencias

- **KML (Kernel Machine Learning)**: Framework para ejecutar modelos ML en el kernel Linux
- **TorchScript**: Formato de PyTorch para exportar modelos a C++
- **StandardScaler**: NormalizaciÃ³n z-score: `(x - mean) / std`
- **LTTng**: Linux Trace Toolkit para captura de eventos del kernel
- **FIO**: Flexible I/O Tester para generaciÃ³n de cargas de trabajo

---

**Ãšltima actualizaciÃ³n**: Noviembre 2025  
**Estado**: Modelo entrenado y listo para integraciÃ³n en kernel âœ…
