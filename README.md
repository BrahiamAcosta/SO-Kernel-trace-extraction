# SO-Kernel-trace-extraction
## Pipeline de ML para Clasificaci√≥n de Patrones de I/O

Sistema de aprendizaje autom√°tico para clasificar patrones de acceso a disco (secuencial, aleatorio, mixto) y optimizar el readahead en el kernel Linux mediante KML (Kernel Machine Learning).

---

## üìã Resumen Ejecutivo

Este proyecto desarrolla un componente de red neuronal que clasifica patrones de I/O en tiempo real dentro del kernel Linux. El objetivo es predecir el tipo de patr√≥n de acceso (sequential, random, mixed) para ajustar din√°micamente el valor de readahead y mejorar el rendimiento del sistema de archivos.

**Contexto del Proyecto:** Integraci√≥n de ML en el kernel Linux usando el framework KML (Kernel Machine Learning) para ajustar autom√°ticamente par√°metros de I/O en tiempo real.

**Versi√≥n del Dataset:** Ventanas de 2.5 segundos (optimizado para balance entre granularidad y estabilidad)

### Flujo General del Proyecto

```
1. Dataset consolidado (CSV con caracter√≠sticas pre-calculadas)
   ‚Üì
2. Procesamiento y normalizaci√≥n de datos
   ‚Üì
3. Entrenamiento de red neuronal ligera
   ‚Üì
4. Exportaci√≥n a formato TorchScript
   ‚Üì
5. Integraci√≥n en kernel Linux mediante KML
   ‚Üì
6. Inferencia en tiempo real para ajustar readahead
```

**Tu responsabilidad**: Pasos 1-4 (desarrollo del modelo ML)  
**Compa√±ero**: Pasos 5-6 (integraci√≥n en kernel)

---

## üìä Descripci√≥n del Dataset

### Metadata del Dataset

| Propiedad | Valor |
|-----------|-------|
| **Archivo** | `consolidated_dataset.csv` |
| **Filas totales** | ~866 (18 runs √ó 48 ventanas) |
| **Columnas** | 40 features + 1 label |
| **Tipo de problema** | Clasificaci√≥n multiclase (3 clases) |
| **Clases balanceadas** | S√≠: 288 filas por clase |
| **Granularidad temporal** | Ventanas de 2.5 segundos |
| **Duraci√≥n por run** | 120 segundos (48 ventanas por run) |
| **Formato** | CSV con header |

### Origen de los Datos

Los datos fueron capturados mediante:
- **LTTng (Linux Trace Toolkit)**: Captura de eventos del kernel (block layer, I/O scheduler, page cache)
- **FIO (Flexible I/O Tester)**: Generaci√≥n de cargas sint√©ticas de trabajo controladas
- **Sistema**: Ubuntu Server con kernel Linux 6.x
- **Hardware**: Disco de prueba, 2 CPU cores, memoria variable

**Configuraci√≥n de captura:**
- Runtime por patr√≥n: 120 segundos
- Repeticiones: 3 runs por patr√≥n
- Modos: cold (cache vac√≠o) y warm (cache pre-cargado)
- Direct I/O: habilitado (bypass page cache)

### Distribuci√≥n de Clases

```
Label         Filas    Descripci√≥n                          Par√°metros FIO
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
sequential    288      Acceso secuencial                    bs=128k, iodepth=4
                       (lectura lineal continua)            
                       
random        288      Acceso aleatorio                     bs=4k, iodepth=16
                       (random reads 4KB)                   
                       
mixed         288      Patr√≥n mixto                         bs=64k, iodepth=8
                       (70% read, 30% write, randrw)        
```

**Balance perfecto:** 288 samples por clase (33.3% cada una) - no requiere t√©cnicas de balanceo.

### Justificaci√≥n de Ventanas de 2.5 Segundos

| Criterio | 5 segundos | **2.5 segundos** ‚≠ê | 1 segundo |
|----------|------------|---------------------|-----------|
| **Samples totales** | 433 | **866** | 2,160 |
| **Estabilidad estad√≠stica** | Muy alta | **Alta** | Media-Baja |
| **Capacidad de reacci√≥n** | Lenta (5s lag) | **Balanceada (2.5s lag)** | R√°pida (1s lag) |
| **Overhead en producci√≥n** | Muy bajo | **Bajo** | Medio-Alto |
| **Riesgo de thrashing** | Muy bajo | **Bajo** | Alto |
| **Adecuado para entrenar** | Ajustado | **√ìptimo** | Excelente |
| **Realismo en producci√≥n** | Conservador | **Pr√°ctico** | Agresivo |

**Conclusi√≥n:** 2.5 segundos ofrece el mejor balance entre suficientes datos para entrenar modelos robustos, features estad√≠sticamente significativas, latencia de adaptaci√≥n aceptable y bajo overhead computacional.

### Caracter√≠sticas Cr√≠ticas del Dataset

Las **5 caracter√≠sticas** seleccionadas para el modelo son:

1. **`trace_avg_sector_distance`** (Feature #1 en importancia) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Distancia promedio entre sectores consecutivos (sectores de 512B)
   - **Sequential:** ~5-20 sectores (4-8 KB)
   - **Random:** ~30,000-80,000 sectores (20+ MB)
   - **Mixed:** ~500-5,000 sectores (500KB-1.5MB)

2. **`trace_sector_jump_ratio`** (Feature #2 en importancia) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Ratio de saltos >1MB entre accesos consecutivos (0.0-1.0)
   - **Sequential:** ~0.00-0.10 (1-5%)
   - **Random:** ~0.80-0.98 (85-95%)
   - **Mixed:** ~0.30-0.60 (40-60%)

3. **`bw_mean_kbps`** (Bandwidth promedio) ‚≠ê‚≠ê‚≠ê‚≠ê
   - **Sequential:** ~400,000-600,000 KB/s (400-600 MB/s)
   - **Random:** ~8,000-20,000 KB/s (10-20 MB/s)
   - **Mixed:** ~100,000-300,000 KB/s (100-300 MB/s)

4. **`lat_mean_ns`** (Latencia promedio) ‚≠ê‚≠ê‚≠ê‚≠ê
   - **Sequential:** ~2,000,000-3,000,000 ns (2-3 ms)
   - **Random:** ~8,000,000-15,000,000 ns (8-15 ms)
   - **Mixed:** ~4,000,000-8,000,000 ns (4-8 ms)

5. **`iops_mean`** (IOPS promedio) ‚≠ê‚≠ê‚≠ê
   - **Sequential:** ~300-500 ops/s
   - **Random:** ~2,000-5,000 ops/s
   - **Mixed:** ~1,000-3,000 ops/s

**‚ö†Ô∏è IMPORTANTE:** Las caracter√≠sticas `bs` y `iodepth` est√°n **excluidas** del modelo porque est√°n fuertemente correlacionadas con el label (riesgo de data leakage). En producci√≥n, no conocer√°s estos par√°metros, solo puedes observar el comportamiento.

---

## üèóÔ∏è Estructura del C√≥digo

### Archivos Principales

#### `build_dataset_from_consolidated.py`
**¬øQu√© hace?**  
Procesa el dataset consolidado (`consolidated_dataset.csv`) y prepara los datos para entrenamiento.

**Funcionamiento:**
1. Lee el CSV con caracter√≠sticas ya calculadas por ventana
2. Mapea las columnas del CSV a las 5 caracter√≠sticas que necesita el modelo:
   - `trace_avg_sector_distance * 512` ‚Üí Distancia promedio (bytes)
   - `trace_sector_jump_ratio` ‚Üí Variabilidad
   - `bw_mean_kbps / iops_mean` ‚Üí Tama√±o promedio I/O (bytes)
   - `1 - trace_sector_jump_ratio` ‚Üí Ratio secuencial
   - `iops_mean` ‚Üí Tasa de I/O (IOPS)
3. Mapea etiquetas de texto (`sequential`, `random`, `mixed`) a n√∫meros (0, 1, 2)
4. Divide los datos en train/test (80/20) de forma estratificada
5. Normaliza las caracter√≠sticas usando `StandardScaler`
6. Guarda:
   - `data/processed/train.npz` y `test.npz` (datos normalizados)
   - `artifacts/scaler.pkl` (normalizador - **CR√çTICO para kernel**)
   - `artifacts/metadata.json` (metadatos del dataset)

**Por qu√© estas 5 caracter√≠sticas?**  
Capturan los aspectos distintivos de cada patr√≥n de forma eficiente y son computacionalmente baratas de calcular en tiempo real dentro del kernel.

#### `neuronal_red.py`
**¬øQu√© hace?**  
Define la arquitectura de la red neuronal.

**Arquitectura:**
```python
Input (5 caracter√≠sticas) 
  ‚Üí Capa Densa 1: 5 ‚Üí 32 neuronas + ReLU + Dropout(20%)
  ‚Üí Capa Densa 2: 32 ‚Üí 16 neuronas + ReLU
  ‚Üí Capa Densa 3: 16 ‚Üí 3 neuronas (logits)
  ‚Üí Salida: [score_sequential, score_random, score_mixed]
```

**¬øPor qu√© es "ligera"?**
- Solo 3 capas densas (no es una red profunda)
- M√°ximo 32 neuronas por capa
- Tama√±o total: ~15 KB
- Inferencia r√°pida (microsegundos)
- Optimizada para ejecuci√≥n en kernel donde los recursos son limitados

**Componentes:**
- `ReLU`: Funci√≥n de activaci√≥n que introduce no-linealidad
- `Dropout(0.2)`: Regularizaci√≥n que previene sobreajuste (desactiva 20% de neuronas aleatoriamente durante entrenamiento)
- `CrossEntropyLoss`: Funci√≥n de p√©rdida para clasificaci√≥n multi-clase

#### `train.py`
**¬øQu√© hace?**  
Entrena la red neuronal y exporta el modelo en formatos compatibles con el kernel.

**Proceso de entrenamiento:**
1. Carga los datos de entrenamiento y prueba
2. Crea un `DataLoader` con batches de 128 muestras
3. Inicializa el modelo, optimizador (Adam) y funci√≥n de p√©rdida
4. Entrena durante hasta 60 √©pocas con:
   - **Early stopping**: Se detiene si no mejora en 8 √©pocas consecutivas
   - **Validaci√≥n**: Eval√∫a en el conjunto de prueba cada √©poca
   - **Mejor modelo**: Guarda el modelo con mejor accuracy en validaci√≥n
5. Exporta el modelo en dos formatos:
   - `model.pth`: Pesos PyTorch (para Python)
   - `model_ts.pt`: **TorchScript** (para C/C++ y kernel) ‚≠ê **PRINCIPAL**
   - `model.onnx`: ONNX (opcional, si se requiere)

**Par√°metros de entrenamiento:**
- Learning rate: 0.001
- Batch size: 128
- Optimizador: Adam
- Early stopping: Paciencia de 8 √©pocas

#### `evaluate.py`
**¬øQu√© hace?**  
Eval√∫a el modelo entrenado y genera m√©tricas de rendimiento.

**M√©tricas generadas:**
- Accuracy general
- Matriz de confusi√≥n (muestra errores por clase)
- Guarda resultados en `artifacts/eval_summary.json`

---

## üöÄ C√≥mo Ejecutar el Pipeline Completo

### 1. Instalaci√≥n de Dependencias

```bash
pip install -r requirements.txt
```

**Dependencias principales:**
- `torch`: Framework de deep learning
- `numpy`, `pandas`: Manipulaci√≥n de datos
- `scikit-learn`: Normalizaci√≥n y divisi√≥n de datos
- `joblib`: Guardar/cargar el normalizador

### 2. Preparar el Dataset

**Requisitos del CSV:**
- Archivo: `consolidated_dataset.csv` en el directorio ra√≠z
- Debe tener una columna `label` con valores: `sequential`, `random`, `mixed`
- Debe contener las columnas necesarias para calcular las 5 caracter√≠sticas

**Ejecutar:**
```bash
python build_dataset_from_consolidated.py
```

**Salida esperada:**
```
Dataset procesado exitosamente!
  - Train: 691 muestras
  - Test: 173 muestras
  - Features: 5
  - Clases: 3
```

**Archivos generados:**
- `data/processed/train.npz` - Datos de entrenamiento normalizados
- `data/processed/test.npz` - Datos de prueba normalizados
- `artifacts/scaler.pkl` - Normalizador (necesario para kernel)
- `artifacts/metadata.json` - Metadatos del dataset

### 3. Entrenar el Modelo

```bash
python train.py
```

**Salida esperada:**
```
Epoch 001 | loss=1.0957 | val_acc=0.3353
Epoch 002 | loss=1.0646 | val_acc=0.3353
...
Epoch 031 | loss=0.1378 | val_acc=0.9711
Early stopping por paciencia.
Entrenamiento completo. Accuracy test=0.9711. Artefactos en 'artifacts/'.
```

**Archivos generados:**
- `artifacts/model.pth` - Pesos PyTorch
- `artifacts/model_ts.pt` - **TorchScript (PARA KERNEL)** ‚≠ê
- `artifacts/training_summary.json` - Resumen del entrenamiento

### 4. Evaluar el Modelo

```bash
python evaluate.py
```

**Salida esperada:**
```
Accuracy test: 0.9711
Matriz de confusi√≥n (filas=verdadero, columnas=predicho):
[[55  0  2]
 [ 0 58  0]
 [ 1  2 55]]
```

**Archivo generado:**
- `artifacts/eval_summary.json` - M√©tricas de evaluaci√≥n

---

## üìä Resultados del Modelo

- **Accuracy en test**: 97.11%
- **Distribuci√≥n de clases**: Balanceada (288 muestras por clase)
- **Tama√±o del modelo**: ~15 KB (TorchScript)
- **Tiempo de inferencia**: Microsegundos (optimizado para kernel)

### Matriz de Confusi√≥n
```
                Predicho
              Seq  Rand  Mix
Real Seq       55    0    2
Real Rand       0   58    0
Real Mixed      1    2   55
```

- **Sequential**: 96.5% correctos
- **Random**: 100% correctos
- **Mixed**: 94.8% correctos

---

## üîÆ C√≥mo Usar el Modelo para Hacer Predicciones

**‚ö†Ô∏è IMPORTANTE:** La red neuronal **NO** calcula las caracter√≠sticas autom√°ticamente. T√∫ debes:
1. **Calcular las 5 caracter√≠sticas** desde tus datos de I/O raw
2. **Normalizar** las caracter√≠sticas usando el scaler
3. **Pasar** las caracter√≠sticas normalizadas a la red neuronal

La red neuronal solo recibe las 5 caracter√≠sticas ya calculadas y las clasifica.

### Ejemplo R√°pido con Python

```python
import joblib
import numpy as np
import torch
from neuronal_red import IOPatternClassifier

# 1. Cargar modelo y scaler
scaler = joblib.load("artifacts/scaler.pkl")
model = IOPatternClassifier(input_size=5, hidden_size=32, num_classes=3)
model.load_state_dict(torch.load("artifacts/model.pth", map_location="cpu"))
model.eval()

# 2. Preparar caracter√≠sticas (ejemplo: patr√≥n secuencial)
# ‚ö†Ô∏è NOTA: Estas caracter√≠sticas DEBEN calcularse desde tus datos de I/O raw
# La red neuronal NO las calcula autom√°ticamente
# 
# Ejemplo de c√°lculo:
# - offsets = [0, 131072, 262144, ...]  # offsets de operaciones I/O
# - avg_distance = promedio(|offsets[i+1] - offsets[i]|) * 512
# - jump_ratio = % de saltos > 1MB
# - etc.
#
# Aqu√≠ usamos valores ya calculados (basados en el dataset real):
features = np.array([
    102774272.0,   # [0] Distancia promedio: 200,731 sectores √ó 512 bytes
    0.14,          # [1] Variabilidad (jump ratio)
    69436416.0,    # [2] Tama√±o promedio I/O: (67,809 KB/s √ó 1024) / 1.0 IOPS
    0.86,          # [3] Ratio secuencial: 1.0 - 0.14
    1.0            # [4] IOPS
], dtype=np.float32)

# 3. CR√çTICO: Normalizar las caracter√≠sticas
features_normalized = scaler.transform(features.reshape(1, -1))

# 4. Hacer predicci√≥n
features_tensor = torch.tensor(features_normalized, dtype=torch.float32)
with torch.no_grad():
    logits = model(features_tensor)
    probabilities = torch.softmax(logits, dim=1)
    predicted_class = torch.argmax(logits, dim=1).item()

# 5. Interpretar resultado
class_map = {0: "sequential", 1: "random", 2: "mixed"}
predicted_label = class_map[predicted_class]
confidence = probabilities[0][predicted_class].item()

print(f"Predicci√≥n: {predicted_label} (confianza: {confidence*100:.2f}%)")
```

### Script de Ejemplo Completo

Ejecuta el script `predict.py` para ver ejemplos completos con los tres patrones:

```bash
python predict.py
```

Este script muestra c√≥mo hacer predicciones con valores reales del dataset.

### Valores de Ejemplo por Patr√≥n

**Para probar con datos similares a los del entrenamiento, usa estos valores:**

#### Patr√≥n Secuencial
```python
avg_sector_distance = 200731      # sectores
sector_jump_ratio = 0.14          # 14%
bw_mean_kbps = 67809             # KB/s
iops_mean = 1.0                   # ops/s
```

#### Patr√≥n Aleatorio
```python
avg_sector_distance = 19534728    # sectores
sector_jump_ratio = 0.998          # 99.8%
bw_mean_kbps = 7518              # KB/s
iops_mean = 1.0                   # ops/s
```

#### Patr√≥n Mixto
```python
avg_sector_distance = 7183669     # sectores
sector_jump_ratio = 0.994          # 99.4%
bw_mean_kbps = 39695             # KB/s
iops_mean = 1.0                   # ops/s
```

**‚ö†Ô∏è IMPORTANTE:** 
- Estos valores est√°n basados en el dataset de entrenamiento
- En producci√≥n, calcula las caracter√≠sticas desde tus datos reales de I/O
- El IOPS en el dataset es constante (1.0), pero en producci√≥n deber√≠as calcular el IOPS real
- Siempre normaliza las caracter√≠sticas antes de pasarlas al modelo

---

## üîß Integraci√≥n en el Kernel Linux

### Contexto: ¬øQu√© necesita hacer tu compa√±ero?

El objetivo final es que el modelo se ejecute dentro del kernel Linux para clasificar patrones de I/O en tiempo real y ajustar el readahead din√°micamente.

### Archivos para Entregar

1. **`artifacts/model_ts.pt`** (14.8 KB) ‚≠ê **PRINCIPAL**
   - Modelo en formato TorchScript
   - Formato compatible con C/C++ y KML
   - Se carga directamente en el kernel

2. **`artifacts/scaler.pkl`** (719 bytes) ‚≠ê **CR√çTICO**
   - Contiene los par√°metros de normalizaci√≥n (medias y desviaciones est√°ndar)
   - **NO se carga directamente**, pero sus par√°metros deben implementarse en C
   - Las caracter√≠sticas DEBEN normalizarse antes de cada inferencia

3. **`artifacts/metadata.json`**
   - Mapeo de clases: `{0: "sequential", 1: "random", 2: "mixed"}`
   - Dimensiones: 5 caracter√≠sticas de entrada, 3 clases de salida
   - Referencia para implementaci√≥n

### Proceso de Integraci√≥n (Responsabilidad del compa√±ero)

#### Paso 1: Cargar el Modelo TorchScript
- Usar la biblioteca de KML o wrapper de TorchScript para C
- Cargar `model_ts.pt` en memoria del kernel
- Inicializar el modelo para inferencia

#### Paso 2: Implementar Normalizaci√≥n en C
- Extraer par√°metros del `scaler.pkl` (medias y desviaciones est√°ndar)
- Implementar normalizaci√≥n en C:
  ```c
  normalized_feature[i] = (feature[i] - mean[i]) / std[i]
  ```
- Aplicar a las 5 caracter√≠sticas antes de cada inferencia

#### Paso 3: Extraer Caracter√≠sticas en Tiempo Real
- Interceptar operaciones de I/O en el kernel
- Calcular las 5 caracter√≠sticas por ventana deslizante:
  1. Distancia promedio entre offsets
  2. Variabilidad (jump ratio)
  3. Tama√±o promedio de I/O
  4. Ratio secuencial
  5. IOPS
- Normalizar usando los par√°metros del scaler

#### Paso 4: Ejecutar Inferencia
- Pasar las 5 caracter√≠sticas normalizadas al modelo
- Obtener los 3 logits (scores) de salida
- Seleccionar la clase con mayor score

#### Paso 5: Mapear a Readahead
- Mapear clase predicha a valor de readahead:
  - `0 (sequential)` ‚Üí Readahead alto (ej: 128-256 KB)
  - `1 (random)` ‚Üí Readahead bajo (ej: 16-32 KB)
  - `2 (mixed)` ‚Üí Readahead intermedio (ej: 64-128 KB)
- Ajustar el readahead del sistema de archivos

### Consideraciones T√©cnicas para el Kernel

1. **Memoria limitada**: El modelo es ligero (~15 KB) para no consumir mucha memoria del kernel
2. **Latencia baja**: La inferencia debe ser r√°pida (microsegundos) para no afectar el rendimiento
3. **Normalizaci√≥n obligatoria**: Las caracter√≠sticas DEBEN normalizarse igual que en entrenamiento
4. **Ventana deslizante**: Las caracter√≠sticas se calculan sobre ventanas de operaciones de I/O
5. **Determinismo**: El modelo es determin√≠stico (sin operaciones aleatorias) para comportamiento predecible

### Formato de Entrada para el Modelo

**Input**: Array de 5 valores float32 normalizados
```c
float features[5] = {
    normalized_avg_distance,
    normalized_variability,
    normalized_avg_io_size,
    normalized_seq_ratio,
    normalized_iops
};
```

**Output**: Array de 3 logits (scores)
```c
float logits[3] = {
    score_sequential,  // Clase 0
    score_random,      // Clase 1
    score_mixed        // Clase 2
};
// Clase predicha = √≠ndice del m√°ximo valor
```

### Valores Reales de Referencia para Predicciones

**‚ö†Ô∏è IMPORTANTE:** Estos son los valores reales del dataset de entrenamiento. Usa valores similares para obtener predicciones confiables.

#### Patr√≥n Secuencial (Sequential)
```python
# Valores t√≠picos basados en el dataset real:
avg_sector_distance = 200731      # Mediana: ~200,731 sectores (~100 MB)
sector_jump_ratio = 0.14           # Mediana: 0.14 (14% de saltos grandes)
bw_mean_kbps = 67809              # Mediana: ~67,809 KB/s (~66 MB/s)
iops_mean = 1.0                   # IOPS promedio

# Caracter√≠sticas calculadas:
feature_1 = 200731 * 512 = 102,774,272 bytes      # Distancia promedio
feature_2 = 0.14                                   # Variabilidad
feature_3 = (67809 * 1024) / 1.0 = 69,436,416 bytes  # Tama√±o promedio I/O
feature_4 = 1.0 - 0.14 = 0.86                      # Ratio secuencial
feature_5 = 1.0                                    # IOPS
```

#### Patr√≥n Aleatorio (Random)
```python
# Valores t√≠picos basados en el dataset real:
avg_sector_distance = 19534728    # Mediana: ~19,534,728 sectores (~9.5 GB)
sector_jump_ratio = 0.998          # Mediana: 0.998 (99.8% de saltos grandes)
bw_mean_kbps = 7518              # Mediana: ~7,518 KB/s (~7.3 MB/s)
iops_mean = 1.0                  # IOPS promedio

# Caracter√≠sticas calculadas:
feature_1 = 19534728 * 512 = 10,001,780,736 bytes    # Distancia promedio
feature_2 = 0.998                                     # Variabilidad
feature_3 = (7518 * 1024) / 1.0 = 7,698,432 bytes    # Tama√±o promedio I/O
feature_4 = 1.0 - 0.998 = 0.002                       # Ratio secuencial
feature_5 = 1.0                                       # IOPS
```

#### Patr√≥n Mixto (Mixed)
```python
# Valores t√≠picos basados en el dataset real:
avg_sector_distance = 7183669     # Mediana: ~7,183,669 sectores (~3.5 GB)
sector_jump_ratio = 0.994          # Mediana: 0.994 (99.4% de saltos grandes)
bw_mean_kbps = 39695             # Mediana: ~39,695 KB/s (~38.7 MB/s)
iops_mean = 1.0                  # IOPS promedio

# Caracter√≠sticas calculadas:
feature_1 = 7183669 * 512 = 3,678,038,528 bytes      # Distancia promedio
feature_2 = 0.994                                    # Variabilidad
feature_3 = (39695 * 1024) / 1.0 = 40,647,680 bytes # Tama√±o promedio I/O
feature_4 = 1.0 - 0.994 = 0.006                      # Ratio secuencial
feature_5 = 1.0                                      # IOPS
```

**Rangos de Valores Observados en el Dataset:**

| Caracter√≠stica | Sequential | Random | Mixed |
|----------------|------------|--------|-------|
| **Distancia promedio (sectores)** | 3,702 - 11,747,540 | 4,915 - 35,373,190 | 7,716 - 27,706,060 |
| **Jump ratio** | 0.00 - 0.45 | 0.76 - 1.00 | 0.96 - 1.00 |
| **Bandwidth (KB/s)** | 19,777 - 182,975 | 1,977 - 23,407 | 3,657 - 384,524 |
| **IOPS** | 1.0 (constante) | 1.0 (constante) | 1.0 (constante) |

**Nota sobre IOPS:** En este dataset, `iops_mean` es constante (1.0) para todas las muestras. En producci√≥n, deber√≠as calcular el IOPS real basado en el n√∫mero de operaciones por segundo observadas en tu ventana de tiempo.

---

## üêõ Soluci√≥n de Problemas

### Error: "No se encontraron trazas en 'data/raw'"
- **Soluci√≥n**: Aseg√∫rate de tener `consolidated_dataset.csv` en el directorio ra√≠z

### Error: "No module named 'torch'"
- **Soluci√≥n**: Instala las dependencias: `pip install -r requirements.txt`

### Warning: "exportaci√≥n ONNX fall√≥"
- **No es cr√≠tico**: TorchScript es el formato principal. ONNX es opcional.

### El modelo tiene baja accuracy
- Verifica que el dataset est√© balanceado
- Revisa que las caracter√≠sticas se est√©n calculando correctamente
- Considera ajustar hiperpar√°metros en `train.py`

---

## üìù Notas Importantes

1. **Normalizaci√≥n es CR√çTICA**: El modelo fue entrenado con datos normalizados. Sin normalizaci√≥n, las predicciones ser√°n incorrectas.

2. **Orden de caracter√≠sticas**: Las 5 caracter√≠sticas deben pasarse en el mismo orden:
   - [0] Distancia promedio
   - [1] Variabilidad
   - [2] Tama√±o promedio I/O
   - [3] Ratio secuencial
   - [4] IOPS

3. **Ventana deslizante**: Las caracter√≠sticas se calculan sobre ventanas de operaciones de I/O. El tama√±o de ventana y overlap deben ser consistentes.

4. **TorchScript es el formato principal**: Aunque se exporta ONNX, TorchScript (`model_ts.pt`) es el formato recomendado para integraci√≥n en kernel.

5. **Exclusi√≥n de features con data leakage**: Las caracter√≠sticas `bs` y `iodepth` est√°n excluidas del modelo porque est√°n fuertemente correlacionadas con el label. En producci√≥n, solo puedes observar el comportamiento, no los par√°metros de configuraci√≥n.

---

## üìö Referencias

- **KML (Kernel Machine Learning)**: Framework para ejecutar modelos ML en el kernel Linux
- **TorchScript**: Formato de PyTorch para exportar modelos a C++
- **StandardScaler**: Normalizaci√≥n z-score: `(x - mean) / std`
- **LTTng**: Linux Trace Toolkit para captura de eventos del kernel
- **FIO**: Flexible I/O Tester para generaci√≥n de cargas de trabajo

---

**√öltima actualizaci√≥n**: Noviembre 2025  
**Estado**: Modelo entrenado y listo para integraci√≥n en kernel ‚úÖ
